---
title: 'Final Project: The S&P 500 in Context'
author:
- Yashna Gupta
- Anika Bastin
- Tanvi Pabbathi
date: "May 4, 2025"
output:
  pdf_document:
    toc: true
    toc_depth: '4'
  html_document:
    highlight: haddock
    theme: lumen
    toc: true
    toc_depth: 4
    toc_float:
      collapsed: false
    number_sections: false
  word_document:
    toc: true
    toc_depth: '4'
urlcolor: blue
---

```{r Setup, include=FALSE, results='hide', warning=FALSE}
knitr::opts_chunk$set(echo = T, fig.width=8, fig.height=4)
options(scipen = 999, digits = 3)  # controls base R output

# Package setup
if(!require("pacman")) install.packages("pacman")

pacman::p_load(tidyverse, glmnet, coefplot, car, pROC, caret)

install.packages("readxl", repos = "http://cran.us.r-project.org")
library(readxl)

```

\newpage

# Background

The Standard and Poor's 500, or S&P 500, is one of the best-known market indices around, having been around since 1957. It tracks the financial performance of the 500 leading U.S. publicly traded companies. It is widely regarded as an excellent measure of financial health and consumer trust in the overall economy, as well as a representative for nearly 80% of the total marekt capitalization of public companies in the United States. Companies on the S&P are represented with tickers and are weighted as per their market capitalization; indeed, the ten largest members of the S&P alone account for nearly 35% of the index. However, while the S&P has provided a great basis for the market, it has been extremely volatile in the last few months due to the confluence of several economic and social issues. However, there are few truly rigorous, statistically sound predictors for the direction of the S&P based on macroeconomic variables, which are potentially less endogenous to the S&P and could thus serve as better predictors. 

Thus we went into this analysis with several motivating questions. 
* Can we identify how macroeconomic sectors move with and influence each other? Can we isolate sectors that correlate heavily, and what are the logical reasons for this correlation?

* Can we identify macroeconomic characteristics or sectors that heavily influence or correlate with the performance of the S&P 500? For example, can we isolate sectors that have spending that tends to influence the S&P? Are there any sectors that we can track that will allow us to make sound investment strategies?

* Can we isolate specific sectors (e.g. commodities, leisure spending) that are heavily correlated with the SPX? We would also like to be able to make comparisons between sectors to see which will result in a better model for outcome prediction. 

* Can we create a model that will simply be able to predict whether the S&P increased in the next day? We typically view stocks as a random walk, with their being some $p$ that they increase in a given instant, in a fairly Bernoulli manner. Thus, we are interested in seeing whether we can predict this seemingly random sequence with high probability. 

These questions are incredibly valuable to investors and individuals in the finance industry. Especially due to recent economic uncertainties, individuals in the finance industry are in dire need of a good predictor of how stocks will move. Thus, our research has several specific implications for such investors. However, they are also very valuable for individual investors. While most people don't have access to specialized data like Bloomberg Second Measure, many news sources still report on more general macroeconomic indices, like the CNN Fear and Greed Index or general reporting on interest rates. Separate indices are very heavily correlated with the factors that we discuss, which means that there is significant importance in individual investors understanding how different macroeconomic factors are correlated with the performance of the generic SPX.

Thus, some specific questions we believe we can address are as follows. 


* How should I best balance the indicators through which I watch the market to ensure that I have a good perspective on the potential behaviors of the market?

* How should I best balance my own portfolio? Is the SPX a very risky asset to be holding, and are there any sectors that are very complementary with it (move with it) or some that move against it? This question is especially helpful to building a countercyclical portfolio, to ensure that the portfolio minimizes risks in the market. 

* Are stocks truly a completely random walk? Are there any indicators that are consistently reliable predictors despite the recent fluctuations? This question is especially helpful due to recent speculations about the true efficacy of portfolio managers.

# Description of the Problem and Tested Solutions

The first method we used to approach this problem was PCA and k-means Clustering. We thought that this would be a good way to separate out the data to determine which variables move together, as well as what types of relationships may be present in the macroeconomic factors. We first performed PCA on a subset of indices consisting only of commodities and another subset consisting of leisure goods, which we break out in further detail later in the report. We were able to find quite clear splits in PCs for both subsets, with one PC typically consisting of more day-to-day, smaller-ticket or necessity purchases like medicine and clothing. The other PC often included larger-ticket purchases, such as Hotel purchases and catering. Thus, we can observe that similar categorical purchases do tend to move together. We then used k-means to cluster the graph against PC scores and months, which helped give us good intuition for seasonal patterns. The three clusters we elected to use could also be clearly differentiated across PCs, which indicates that the clustering seems to have picked on to the big-ticket/small-ticket spending patterns we were noticing. Additionally, it confirmed our intuition that spending seemed to pick up around holidays, confirming the existence of the well-known "Christmas Effect."

Having explored the relationship between variables in more depth, we then attempted to create a good predictive model for SPX. As we had a very large number of variables, we thought the data was very well suited to LASSO and Multiple Regression methods of modeling. Indeed, with LASSO alone, we were able to create a good prediction model, with a $R^2$ of around 0.95 and a relatively low AIC compared to some of our other models, including the model consisting of the backwards selection that we performed to ensure that all variables were significant. Our final model was quite a bit more parsimonious though, at 23 variables compared to the 113 variables that we started with. In this model, we found that leisure spending categories seemed to have particularly strong $p$ values to predict model fit, as some of the most significant spend variables included specialty food stores, discount department stores, clothing, and more. However, macroeconomic indicators retained their relevance as the Discount Federal Funds Rate and General Consumer Spend tracking lines also retained their importance. Thus, the LASSO model demonstrates good model parsimony and fit.

Second, we decided to answer the question of whether we could create a better model to predict stock market movement from the day before, as compared to simply assuming that stocks are a completely "random walk." For this section, we decided to use logistic regression models to approach this question. We also used boosting to see if we could create a better model, as being able to predict relative movement of the SPX can occasionally be more helpful then being able to determine magnitude of change, especially as there is so much more uncertainty surrounding true magnitude. As is evident, the performance of our boosted model was better; indeed, our boosted model only misclassified stock movement 12 times, with a total accuracy of $94.3\%$, as compared to the $88.6\%$ of Logistic Regression. Additionally, we found very different relationships between our predictor variables and actual stock movement between Logistic and Boosting techniques, as Logistic seemed to place more importance on predictors based in construction, with many manufacturing based variables being in our Top 10. Alternatively, boosting had a much more general array of variables, with no seeming pattern to them, which could have been helpful since there might be less collinearity between these variables. It also utilized more macroeconomic indicators, like TSA and In Store, which could have been helpful to pushing up performance. 

# Description of the Data

The original basis for the dataset was collected from the closing price index for the S&P 500 over the last eight months. This data was obtained from the Bloomberg Terminal. One note to this analysis is that the S&P is not open to trading over the weekends and on holidays. As a result, prices tend to remain constant over those days, which may have affected the interplay between our dependent and independent variables in our final results. We then collated independent variables during these days.

* We pulled macroeconomic indicators, most notably debit card data, from the Bloomberg Second Measure Data. This data provides insights from customer transaction data. Additionally, while Second Measure has an overall index category, it also contains debit card data broken into several different categories, such as Motor Vehicles, Food and Beverage, and more. This could allow more more specifications on what industries are key to driving stock performance. However, there are also a wealth of categories which makes this dataset ideal for analysis with dimensionality reduction tools like LASSO. We also had some intuition on important categories, like tech and food/commodities, which allowed us to perform some subset analysis on thie data. 

* We utilized data on other notable macroeconomic data indices, collated from various sources. One notable to this category included the number of people entering TSA checkpoints daily, which is an excellent indicator of the health of the overall leisure spending category. We also included data on job rates and important economic indices like the Federal Funds Rate, both of which were pulled from the FRED list of indices collated by the St. Louis Federal Reserve Bank. These more generic indices reflect the state of the economy, especially how "hot" it is / how much activity we can observe.

\newpage

# Section 1. EDA
We began by looking at some of the basics of the data, including the distribution of the S&P itself. Additionally, we broke out the data into subsets that only included information relevant to tech and commodities for use in later analysis. We started by cleaning the data, which included converting it, as it needed to be transposed before it could be used. We also removed duplicate columns, as Second Measure has the tendency to include one piece of data several times. This left us with around 113 variables, which I will explore more in depth throughout the EDA process.

```{r, suppressWarnings = TRUE, include=FALSE}
df <- suppressWarnings(readxl::read_excel("/Users/anikabastin/Desktop/second_measure.xlsx"))
df <- df[ , -1]
df <- df[!is.na(df$Series), ]     
df <- df[!duplicated(df$Series), ]
df_n <- df
```


```{r, suppressWarnings = TRUE, include=FALSE}
rownames(df) <- df[[1]]

df_t <- as.data.frame(t(df))

df_t <- df_t[-1, ]

df_t <- df_t[1:212, ]

library(dplyr)


df_t <- df_t %>% mutate(across(everything(), as.numeric))
```
As we can see from the graph of SPX, there seems to be a somewhat rightwards skew on the distribution, with a much larger, more variable tail on this side. This could reflect the fact that the SPX has had a rather tumultuous last couple of months, which has led to there being longer periods of it deviating downwards as compared to normal, more stable economic periods where it may display more normal behavior. ]

Thus, we consider some other methods of making this data appear more normal. Some of the methods we attempted, which are common for addressing the type of skew that we observed, were a logarithmic transform, a square root transform, and a Boxcox Transform. Most of these seemed to somewhat retain the same type of skew that we observed from the untransformed dataset, so we elected to retain the untransformed variable to improve parsimony and simplicity of analysis conclusions. However, we have included below the ggplot histogram representations of all of the transforms to display the similarities

```{r, echo = FALSE, include = FALSE}
library(MASS)

df_t$SPX <- as.numeric(df_t$SPX)

bc <- boxcox(SPX ~ 1, data = df_t)
lambda <- bc$x[which.max(bc$y)]
df_t$boxcox_SPX <- (df_t$SPX^lambda - 1) / lambda 

df_t$log_SPX <- log(df_t$SPX)

df_t$sqrt_SPX <- sqrt(df_t$SPX)
```

```{r, echo = FALSE}
library(ggplot2)
library(ggpubr)

p1 <- ggplot(df_t, aes(x = SPX)) +
  geom_histogram(binwidth = 20, fill = "pink", color = "black") +
  labs(title = "Histogram of SPX", x = "SPX", y = "Count") +
  theme_minimal()

p2 <- ggplot(df_t, aes(x = boxcox_SPX)) +
  geom_histogram(binwidth = 100000, fill = "skyblue", color = "black") +
  labs(title = "Histogram of Boxcox", x = "Boxcox Transform", y = "Count") +
  theme_minimal()

p3 <- ggplot(df_t, aes(x = log_SPX)) +
  geom_histogram(binwidth = 0.004, fill = "limegreen", color = "black") +
  labs(title = "Histogram of Log Transform", x = "Log(SPX)", y = "Count") +
  theme_minimal()

p4 <- ggplot(df_t, aes(x = sqrt_SPX)) +
  geom_histogram(binwidth = 0.1, fill = "lavender", color = "black") +
  labs(title = "Histogram of Square Root", x = "sqrt(SPX)", y = "Count") +
  theme_minimal()

ggarrange(p1, p2, p3, p4, ncol = 2, nrow = 2)

df <- df_t
df <- df[, 1:(ncol(df) - 3)]
```
We also attempted to create a normalized heatmap of our data. However, as we have an extremely large number of variables, we decided to carry out a SRS on the variables to plot a representative heatmap of the data. Additionally, we included a single normalized, base heatmap for reference. This heatmap continaed the variables that were deemed most important by hierarchichal clustering. However, this heatmap is difficult to interpret due to the complexity. We can still see by the colors that many of these variables tend to move together, which makes logical sense. This could have effect on our final model, however, as this could indicate that several of our baseline independent variables may display some high amount of collinearity. We will have to check correlation between the variables, as well as LASSO choices of variables, in order to determine this. However, overall, SPX does seem to move somewhat differently from the other variables displayed here, which could mean that our predictive model will still hold value. 

```{r, echo=FALSE, suppressWarnings = TRUE, message = FALSE}

install.packages("RColorBrewer", repos = "http://cran.us.r-project.org")
library(RColorBrewer)

df_numeric <- df[sapply(df, is.numeric)]
mat <- data.matrix(df_numeric)
mat_scaled <- scale(mat)
par(mfrow = c(1, 2)) 
heatmap(mat_scaled,
        Rowv = NA, Colv = NA,
        col = colorRampPalette(brewer.pal(9, "YlGnBu"))(100),
        cexRow=0.5, cexCol=0.5,
        main = "Normal, Baseline Heatmap")

n_vars <- 30

```

We were also interested in taking a subset only associated with commodities like Food, Oil, and other major necessities for life in America. We also retained some general macroeconomic indices, most notably TSA, the Federal Funds Rate, and statistics related to job seeking activity. We decided to test these as we believed that they would provide more stability than the entire set of data. Thus, we decided to perform some correlation tests, heatmaps, and EDA on this data specifically. It's also a smaller subset of variables, which we would be especially helpful for our logistic regression analysis as compared to the full dataset. 

As we can see from the heatmap, this set of data still seems to retain significant variation in values as many columns of the heatmap do not move together. However, there are still some sections that could retain some potentially problematic collinearity, which we can remove during backwards selection processes. However, some of them do seem to exhibit some quite strong correlation with the SPX data. 
```{r, echo = FALSE}

df.com <- df %>%
  dplyr::select("SPX", "Specialty Food Stores", "Online", "TSA data", "DFF","Consumer Spend", "Food and Beverage Stores", "Water Transportation", "Food Services and Drinking Places", "BFJOB", "In-Store", "Gasoline Stations with Convenience Stores", "Full-Service Restaurants", "Limited-Service Restaurants", "Restaurants and Other Eating Places", "Full-Service Restaurants", "Restaurants and Other Eating Places", "Caterers", "Special Food Services", "Hotels (except Casino Hotels) and Motels", "Grocery Stores", "Supermarkets and Other Grocery (except Convenience) Stores", "Specialty Food Stores", "Other Specialty Food Stores", "Baked Goods Stores", "Confectionery and Nut Stores", "All Other Specialty Food Stores", "Beer, Wine, and Liquor Stores", "Other Gasoline Stations", "Gasoline Stations", "Fuel Dealers", "Heating Oil Dealers", "Pharmacies and Drug Stores")

df_numeric <- df.com[sapply(df.com, is.numeric)]
mat <- data.matrix(df_numeric)
mat_scaled <- scale(mat)

heatmap(mat_scaled,
        Rowv = NA, Colv = NA,
        col = colorRampPalette(brewer.pal(9, "YlGnBu"))(100),
        cexRow=0.5, cexCol=0.5,
        main = "Commodities Subset Heatmap")
```
We then broke out another dataset associated solely with discretionary purchases, as we thought this could also be an interesting comparative to the commodities set that we broke out right before This subset included purchases like Furniture, Clothing, and Jewelry, which are not as necessary to daily life. As a result, this set of purchases fluctuates much more than some of our other predictors, since they are often the first types of spending to be cut during times of economic risk. We thought that up and downturns in this set of variables would be very sensitive to consumer sentiment. 

We can see that this subset seems to be much more consistent in movement. This makes sense as stocks are also seen as a relatively risky, discretionary investment. Thus downwards movement in categories like leisure spending should reasonably be correlated with downturns in investing sentiment. 

```{r, echo = FALSE}
df.disc <- df %>%
  dplyr::select("Furniture and Home Furnishings Stores","Electronics and Appliance Stores","Clothing and Clothing Accessories Stores","Sporting Goods, Hobby, Musical Instrument, and Book Stores", "Air Transportation","Motor Vehicle and Parts Dealers","Automobile Dealers","New Car Dealers","Recreational Vehicle Dealers","Motorcycle, Boat, and Other Motor Vehicle Dealers","Motorcycle, ATV, and All Other Motor Vehicle Dealers","Automotive Parts, Accessories, and Tire Stores","Automotive Parts and Accessories Stores","Furniture Stores","Paint and Wallpaper Stores","Lawn and Garden Equipment and Supplies Stores","Nursery, Garden Center, and Farm Supply Stores","Cosmetics, Beauty Supplies, and Perfume Stores","Optical Goods Stores",
  "Other Health and Personal Care Stores",
  "All Other Health and Personal Care Stores",
  "Clothing Stores",
  "Other Clothing Stores",
  "Shoe Stores",
  "Jewelry, Luggage, and Leather Goods Stores",
  "Jewelry Stores",
  "Luggage and Leather Goods Stores",
  "Sporting Goods, Hobby, and Musical Instrument Stores",
  "Sporting Goods Stores",
  "Hobby, Toy, and Game Stores",
  "Sewing, Needlework, and Piece Goods Stores",
  "Musical Instrument and Supplies Stores",
  "Book, Periodical, and Music Stores",
  "Book Stores",
  "Department Stores",
  "Department Stores Except Discount Department Stores",
  "Florists",
  "Office Supplies, Stationery, and Gift Stores",
  "Office Supplies and Stationery Stores",
  "Gift, Novelty, and Souvenir Stores",
  "Other Miscellaneous Store Retailers",
  "Pet and Pet Supplies Stores",
  "Art Dealers",
  "Tobacco Stores",
  "All Other Miscellaneous Store Retailers (except Tobacco Stores)",
  "Scheduled Air Transportation",
  "Scheduled Passenger Air Transportation",
  "Deep Sea, Coastal, and Great Lakes Water Transportation",
  "Deep Sea Passenger Transportation",
  "Traveler Accommodation",
  "Casino Hotels",
  "Other Traveler Accommodation",
  "All Other Traveler Accommodation")
  
df_numeric <- df.disc[sapply(df.disc, is.numeric)]
mat_disc <- data.matrix(df_numeric)
mat_disc_scaled <- scale(mat_disc)

heatmap(mat_disc_scaled,
        Rowv = NA, Colv = NA,
        col = colorRampPalette(brewer.pal(9, "YlGnBu"))(100),
        cexRow=0.5, cexCol=0.5,
        main = "Discretionary Subset Heatmap")  


```

Having some idea about the interplay of variables, we then moved into looking more at our baselines for variables. We arranged the commodity and Leisure Spending Boxplots. From seeing these, it is evident that the Commodity Spending categories tend to move together relatively tightly. The major exception to this is the Transportation variable, which has a much higher median than the others. Even so, it has a relatively large spread, which indicates more variability in the variable overall. Additionally, we can see variables associated with transportation in general, such as gas stations, tend to have higher levels of spending. This makes logical sense as oil/transportation support does tend to be costlier than food expenses. 

```{r, echo = FALSE}

df.com <- df %>%
  dplyr::select("Specialty Food Stores", "Online","Consumer Spend", "Food and Beverage Stores", "Water Transportation", "Food Services and Drinking Places", "In-Store", "Gasoline Stations with Convenience Stores", "Full-Service Restaurants", "Limited-Service Restaurants", "Restaurants and Other Eating Places", "Full-Service Restaurants", "Restaurants and Other Eating Places", "Caterers", "Special Food Services", "Hotels (except Casino Hotels) and Motels", "Grocery Stores", "Supermarkets and Other Grocery (except Convenience) Stores", "Specialty Food Stores", "Other Specialty Food Stores", "Baked Goods Stores", "Confectionery and Nut Stores", "All Other Specialty Food Stores", "Beer, Wine, and Liquor Stores", "Other Gasoline Stations", "Gasoline Stations", "Fuel Dealers", "Heating Oil Dealers", "Pharmacies and Drug Stores")

df_numeric <- df.com[sapply(df.com, is.numeric)]
mat <- data.matrix(df_numeric)

data_vars <- df.com[ , ]

medians <- apply(data_vars, 2, median, na.rm = TRUE)

ordered_names <- names(sort(medians))

df.com <- data_vars[ , ordered_names]


boxplot(df.com[,], 
        main = "Commodity Spending Box Plots", 
        las = 2,               
        col = rainbow(ncol(df.com)))

```
The Leisure spending boxplot also seemed somewhat similar, in terms of a relatively tight median distribution even though it had significantly more variables in its category. However, one variable notably had an immense number of outliers, which was Sewing and Clothing Goods Stores. This industry is likely relatively volatile, as crafting goods can only be pursued by people with significant amount of leisure time to learn, or those who are already very familiar with the art. However, most of the outliers were in the upwards range. Thus, there could have just been certain days with major upticks, such as holidays or weekends where people had a lot more time and could visit such stores, which resulted in the skewed distribution. We have included another boxplot without that variable to make the image more digestible.

```{r, echo = FALSE}

data_vars <- df.disc[ , ]

medians <- apply(data_vars, 2, median, na.rm = TRUE)

ordered_names <- names(sort(medians))

df.disc <- data_vars[ , ordered_names]

df.discs <- df %>%
  dplyr::select("Furniture and Home Furnishings Stores","Electronics and Appliance Stores","Clothing and Clothing Accessories Stores","Sporting Goods, Hobby, Musical Instrument, and Book Stores", "Air Transportation","Motor Vehicle and Parts Dealers","Automobile Dealers","New Car Dealers","Recreational Vehicle Dealers","Motorcycle, Boat, and Other Motor Vehicle Dealers","Motorcycle, ATV, and All Other Motor Vehicle Dealers","Automotive Parts, Accessories, and Tire Stores","Automotive Parts and Accessories Stores","Furniture Stores","Paint and Wallpaper Stores","Lawn and Garden Equipment and Supplies Stores","Nursery, Garden Center, and Farm Supply Stores","Cosmetics, Beauty Supplies, and Perfume Stores","Optical Goods Stores",
  "Other Health and Personal Care Stores",
  "All Other Health and Personal Care Stores",
  "Clothing Stores",
  "Other Clothing Stores",
  "Shoe Stores",
  "Jewelry, Luggage, and Leather Goods Stores",
  "Jewelry Stores",
  "Luggage and Leather Goods Stores",
  "Sporting Goods, Hobby, and Musical Instrument Stores",
  "Sporting Goods Stores",
  "Hobby, Toy, and Game Stores",
  "Musical Instrument and Supplies Stores",
  "Book, Periodical, and Music Stores",
  "Book Stores",
  "Department Stores",
  "Department Stores Except Discount Department Stores",
  "Florists",
  "Office Supplies, Stationery, and Gift Stores",
  "Office Supplies and Stationery Stores",
  "Gift, Novelty, and Souvenir Stores",
  "Other Miscellaneous Store Retailers",
  "Pet and Pet Supplies Stores",
  "Art Dealers",
  "Tobacco Stores",
  "All Other Miscellaneous Store Retailers (except Tobacco Stores)",
  "Scheduled Air Transportation",
  "Scheduled Passenger Air Transportation",
  "Deep Sea, Coastal, and Great Lakes Water Transportation",
  "Deep Sea Passenger Transportation",
  "Traveler Accommodation",
  "Casino Hotels",
  "Other Traveler Accommodation",
  "All Other Traveler Accommodation")

data_vars <- df.discs[ , ]

medians <- apply(data_vars, 2, median, na.rm = TRUE)

ordered_names <- names(sort(medians))

df.discs <- data_vars[ , ordered_names]

par(mfrow = c(1, 2))

boxplot(df.disc[,], 
        main = "Leisure Spending w/ Sewing Stores", 
        las = 2,               
        col = rainbow(ncol(df.disc)))

boxplot(df.discs[,], 
        main = "Leisure Spending w/o Sewing Stores", 
        las = 2,               
        col = rainbow(ncol(df.discs)))

par(mfrow = c(1, 1))

```

\newpage

# Section 2. PCA and Cluster Analysis

PCA was done on the subset of commodities from the full data set. This subset includes variables like food, gasoline, pharmacies, and hotels. We used these indicators as they are relatively stable with the year. By looking at the PC scores, we confirmed they are orthogonal and independent. From the PVE curve, we see that using two PC scores is enough as it encompases 69.8% of the variability of the data. When interpreting the different PC scores we look at the variables that are segmented into positive and negative PC1 and PC2 scores.
```{r, echo = FALSE, suppressWarnings = TRUE, message=FALSE, warning=FALSE}
library(ggbiplot)
library(data.table)

mat_centered <- scale(mat, center = TRUE, scale = TRUE)
pca <- prcomp(mat_centered, center = FALSE, scale = FALSE)
pc12 <- pca$rotation[, 1:2]
#colSums(pc12^2)
#sum(pc12[, 1] * pc12[, 2])
pc_scores <- pca$x
par(mfrow = c(1, 2)) 
PVE <- pca$sdev^2 / sum(pca$sdev^2)
barplot(PVE, main = "PVE by PC", 
        xlab = "Principal Components", ylab = "Proportion of Variance Explained", 
        col = "lightblue", beside = TRUE)

#summary(PVE)
cpve <- cumsum(PVE)
plot(cpve, type = "b", main = "Cumulative PVE by PC",
     xlab = "Principal Components", ylab = "Cumulative PVE",
     pch = 19, col = "green", lwd = 2)

#sum of variance explained by 2
#sum(PVE[1:2])
#sum of variance explained by 3
#sum(PVE[1:3])

ggbiplot(pca, 
         choices = c(1, 2),  
         obs.scale = 1, 
         var.scale = 2, 
         ellipse = FALSE,     
         circle = TRUE) +     
  theme_minimal() +
  ggtitle("PCA Biplot: PC1 vs PC2")

#plot(pc_scores[,1], pc_scores[,2],
#     xlab = "PC1",
#     ylab = "PC2",
#     main = "PC1 vs PC2",
#     col = "blue", pch = 19)

pc1pos <- pc12[pc12[,1]>0, ]
pc1pos <- pc1pos[order(-pc1pos[,1]),]
pc1neg <- pc12[pc12[,1]<0, ]
pc1neg <- pc1neg[order(pc1neg[,1]),]
pc2pos <- pc12[pc12[,2]>0, ]
pc2pos <- pc2pos[order(-pc2pos[,2]),]
pc2neg <- pc12[pc12[,2]<0, ]
pc2neg <- pc2neg[order(pc2neg[,2]),]
#pc3pos <- pc12[pc12[,3]>0, ]
#pc3pos <- pc3pos[order(-pc3pos[,3]),]
#pc3neg <- pc12[pc12[,3]<0, ]
#pc3neg <- pc3neg[order(pc3neg[,3]),]
pc1pos_dt <- data.table(Variable = rownames(pc1pos), PosPC1 = pc1pos[,1])
pc1neg_dt <- data.table(Variable = rownames(pc1neg), NegPC1 = pc1neg[,1])
pc2pos_dt <- data.table(Variable = rownames(pc2pos), PosPC2 = pc2pos[,2])
pc2neg_dt <- data.table(Variable = rownames(pc2neg), NegPC2 = pc2neg[,2])

# View top 10 strongest contributors
#head(pc1pos_dt, 10)
#head(pc1neg_dt, 10)
library(knitr)
pc2posfin <- head(pc2pos_dt, 10)
pc2negfin <- head(pc2neg_dt, 10)
kable(pc2posfin, caption = "Top 10 Postitive PC2 Variables: Leisure/Occasion Spending")
kable(pc2negfin, caption = "Top 10 Negative PC2 Variables: Everyday/Cyclic Spending")

#pc1pos scores tend to be correlated to pc2neg scores?

#pc2+ scores are necessary spending
#pc2- scores are discretionary spending/leisure.
```




The PC2 scores tend to be positive with more leisure and luxury purchases (such as art dealers and casino hotels), whereas negative PC2 scores tend to be more everyday purchases (such as department stores, clothing stores, and pet supplies). To show this, the first ten most positive and negative variables are plotted. Eventhough variables like musical instruments seem like they may be more similar to a leisure purchase, musicians often have to routinely have their instrument purchased and have to buy supplies to help take care of their instruments. We concluded cyclic and occasion dependent spending were valid general trends.
```{r, echo = FALSE, suppressWarnings = TRUE}
library(ggbiplot)
library(data.table)

mat_centered_d <- scale(mat_disc, center = TRUE, scale = TRUE)
pca <- prcomp(mat_centered_d, center = FALSE, scale = FALSE)
pc12 <- pca$rotation[, 1:2]
#colSums(pc12^2)
#sum(pc12[, 1] * pc12[, 2])
pc_scores <- pca$x
par(mfrow = c(1, 2)) 
PVE <- pca$sdev^2 / sum(pca$sdev^2)
#barplot(PVE, main = "PVE by PC", 
#        xlab = "Principal Components", ylab = "Proportion of Variance Explained", 
#        col = "lightblue", beside = TRUE)

#summary(PVE)
cpve <- cumsum(PVE)
#plot(cpve, type = "b", main = "Cumulative PVE by PC",
#     xlab = "Principal Components", ylab = "Cumulative PVE",
#     pch = 19, col = "green", lwd = 2)

#sum of variance explained by 2
#sum(PVE[1:2])
#sum of variance explained by 3
#sum(PVE[1:3])

ggbiplot(pca, 
         choices = c(1, 2),  
         obs.scale = 1, 
         var.scale = 2, 
         ellipse = FALSE,     
         circle = TRUE) +     
  theme_minimal() +
  ggtitle("PCA Biplot: PC1 vs PC2")

#plot(pc_scores[,1], pc_scores[,2],
#     xlab = "PC1",
#     ylab = "PC2",
#     main = "PC1 vs PC2",
#     col = "blue", pch = 19)

pc1pos <- pc12[pc12[,1]>0, ]
pc1pos <- pc1pos[order(-pc1pos[,1]),]
pc1neg <- pc12[pc12[,1]<0, ]
pc1neg <- pc1neg[order(pc1neg[,1]),]
pc2pos <- pc12[pc12[,2]>0, ]
pc2pos <- pc2pos[order(-pc2pos[,2]),]
pc2neg <- pc12[pc12[,2]<0, ]
pc2neg <- pc2neg[order(pc2neg[,2]),]
#pc3pos <- pc12[pc12[,3]>0, ]
#pc3pos <- pc3pos[order(-pc3pos[,3]),]
#pc3neg <- pc12[pc12[,3]<0, ]
#pc3neg <- pc3neg[order(pc3neg[,3]),]
pc1pos_dt <- data.table(Variable = rownames(pc1pos), PosPC1 = pc1pos[,1])
pc1neg_dt <- data.table(Variable = rownames(pc1neg), NegPC1 = pc1neg[,1])
pc2pos_dt <- data.table(Variable = rownames(pc2pos), PosPC2 = pc2pos[,2])
pc2neg_dt <- data.table(Variable = rownames(pc2neg), NegPC2 = pc2neg[,2])

# View top 10 strongest contributors
#head(pc1pos_dt, 10)
#head(pc1neg_dt, 10)
library(knitr)
pc2posfin <- head(pc2pos_dt, 10)
pc2negfin <- head(pc2neg_dt, 10)
kable(pc2posfin, caption = "Top 10 Postitive PC2 Variables: Leisure/Occasion Spending")
kable(pc2negfin, caption = "Top 10 Negative PC2 Variables: Everyday/Cyclic Spending")

#pc1pos scores tend to be correlated to pc2neg scores?

#pc2+ scores are necessary spending
#pc2- scores are discretionary spending/leisure.
```

kmeans is used to group cluster and graph against PC1 and PC2 scores. The PVE plot shows that 3 clusters are optimal and capture a majority of the variance and hence is the lower amound of clusters that will explain the data well. The plots show each date as a dot. As shown the PC2 scores tend to increase around November and December, especially late november and december where the holiday season tends to ramp up and black friday also occurs. Travel for holidays like Thanksgiving and Christmas, holiday vacations, and gift spending would all contribute to these higher PC2 scores. To further emphasize this Christmas is plotted in lime green, and Thanksgiving is plotted in dark blue. As shown there are spikes around these dates, likely because of increased travel during those holidays. The days leading up to and after Christmas and Thanksgiving are known to be the busiest travel times of the year. As the holidays pass, the PC2 scores dip, showing that there is a decrease in occasion-dependent spending, likely as people are settling back into their lives in the new year and there are not any major holidays coming up.

```{r, echo = FALSE, suppressWarnings = TRUE}
pr <- prcomp(mat_centered)
svd_out <- svd(mat_centered)

svd_ret <- irlba::irlba(mat_centered, nv = 10)
svd_var <- svd_ret$d^2/(nrow(mat_centered)-1)
pve_apx <- svd_var/(nrow(mat_centered))
plot(pve_apx, type="b", pch = 19, frame = FALSE)

pc <- (svd_out$u[, 1:2]) * (svd_out$d[1:2])
kmean_ret <- kmeans(x = pc, 3)
dates <- rownames(mat_centered)
full_dates <- as.Date(dates, format = "%m/%d/%Y")
months <- format(full_dates, "%m")

centroid <- data.table(
          x = kmean_ret$centers[, 1],  
          y = kmean_ret$centers[, 2]   
        )
p_cm <- data.table(x = pc[,1],
                        y = pc[,2],
                        col = as.factor(months),
                        cl = as.factor(kmean_ret$cluster),
                        date = full_dates)
highlight_point <- dplyr::filter(p_cm, date == as.Date("2024-12-25"))
highlight_point1 <- dplyr::filter(p_cm, date == as.Date("2024-11-28"))
 p_centroid_month_1 <-
   ggplot(p_cm) +
          geom_point(aes(x = x, y = y, col = col, shape = cl)) +
          theme_bw() +
          labs(color = "Month", shape = "Cluster") +
          xlab("PC1") +
          ylab("PC2") +
          ggtitle("Commodities Subset")+
          geom_point(data = centroid,
            aes(x = x, y = y),
            color = "black"
          ) +
          geom_point(data = highlight_point, aes(x = x, y = y), color = "green", size = 3) +
          geom_point(data = highlight_point1, aes(x = x, y = y), color = "blue", size = 3)
 
 
prd <- prcomp(mat_centered_d)
svd_outd <- svd(mat_centered_d)
pcd <- (svd_outd$u[, 1:2]) * (svd_outd$d[1:2])
kmean_retd <- kmeans(x = pcd, 3)
        
centroidd <- data.table(
          x = kmean_retd$centers[, 1],  
          y = kmean_retd$centers[, 2]   
        )
p_cmd <- data.table(x = pcd[,1],
                        y = pcd[,2],
                        col = as.factor(months),
                        cl = as.factor(kmean_retd$cluster),
                        date = full_dates)
highlight_pointd <- dplyr::filter(p_cmd, date == as.Date("2024-12-25"))
highlight_point1d <- dplyr::filter(p_cmd, date == as.Date("2024-11-28"))

 p_centroid_monthd <-
   ggplot(p_cmd) +
          geom_point(aes(x = x, y = y, col = col, shape = cl)) +
          theme_bw() +
          labs(color = "Month", shape = "Cluster") +
          xlab("PC1") +
          ylab("PC2") +
          ggtitle("Discretionary Subset")+
          geom_point(data = centroid,
            aes(x = x, y = y),
            color = "black"
          ) +
          geom_point(data = highlight_pointd, aes(x = x, y = y), color = "green", size = 3) +
          geom_point(data = highlight_point1d, aes(x = x, y = y), color = "blue", size = 3)
 
ggarrange(p_centroid_month_1, p_centroid_monthd, ncol =2, nrow = 1) 


#for (i in 1:max(kmean_ret$cluster)) {
#  cat(paste("Cluster", i, ":\n"))
#  print(rownames(mat_centered)[which(kmean_ret$cluster == i)])
#  cat("\n")
#}
#cluster 1 is sept-nov9 2024
#cluster 2 is nov 10 - feb 4
#cluster 3 is feb 4 - mar 31
```

\newpage
# Section 2. LASSO for Multiple Regression

For our first model, we want to perform multiple regression on our dataset with a goal of predicting SPX. Additionally, since we have a large number of features in our dataset, we chose to use LASSO as a shrinkage method. 

```{r, echo = FALSE, include = FALSE}
library(glmnet)
library(dplyr)
```

```{r, echo = FALSE}
set.seed(123)
colnames(df) <- make.names(colnames(df))

Y <- as.matrix(df[, 'SPX'])
X <- model.matrix(~ ., data = df[, colnames(df) != "SPX"])
cv_model <- cv.glmnet(X, Y, alpha = 1)
plot(cv_model)
```

```{r, echo = FALSE}
best_lambda <- cv_model$lambda.min
lasso_model <- glmnet(X, Y, alpha = 1, lambda = best_lambda)
coef.force.min <- coef(lasso_model)
var.min <- rownames(as.matrix(coef.force.min[which(coef.force.min != 0),][-1])) 

lasso_sub <- dplyr::select(df, SPX, all_of(var.min))

model2_lasso <- lm(SPX ~ ., data = lasso_sub)

s <- summary(model2_lasso)
coefs <- s$coefficients
coefs_no_intercept <- coefs[rownames(coefs) != "(Intercept)", ]
ordered_coefs <- coefs_no_intercept[order(coefs_no_intercept[, 4]), ]
top10 <- head(ordered_coefs, 10)
lasso_selected <- colnames(lasso_sub)
top10_lasso_sub <- lasso_sub[, 1:10]

# Get their column names
lasso_selected <- colnames(top10_lasso_sub)
```

```{r, echo = FALSE}
library(knitr)

# Convert to data frame
lasso_selected_df <- data.frame(Selected_Variables = lasso_selected)

# Pretty table
kable(top10, caption = "Top LASSO-Selected Predictors for SPX")
```

Based on our linear regression, we see that our R^2 value is quite high (meaning our model was good at predicting the data) at 0.95. Now, we use backwards selection to only keep the most significant variables for our model. Additionally, we see that most features have become statistically insiginificant following our LASSO. Features that are still significant include "Building.Material.and.Garden.Equipment.and.Supplies.Dealers", "DFF", etc (there are many, but chose not to list all of them here). 

Next, we chose to use backwards selection as a mechanism to reduce the number of features we are modelling on following LASSO. This way, we are only keeping the most significant features for our model. 
```{r, echo = FALSE}
library(leaps)

df_clean <- df[complete.cases(df), ]

back_model <- regsubsets(SPX ~ ., data = df_clean, nvmax = 30, method = "backward")

summary_back <- summary(back_model)
which.min(summary_back$bic)  

selected_vars <- names(coef(back_model, which.min(summary_back$bic)))
```
The list above shows the selected features to keep following backward selection (25) including the intercept. 
```{r, echo = FALSE}
predictors <- selected_vars[selected_vars != "(Intercept)"]

formula_back <- as.formula(paste("SPX ~", paste(predictors, collapse = " + ")))

model_backward <- lm(formula_back, data = df_clean)
s <- summary(model_backward)
coefs <- s$coefficients
coefs_no_intercept <- coefs[rownames(coefs) != "(Intercept)", ]
ordered_coefs <- coefs_no_intercept[order(coefs_no_intercept[, 4]), ]
top10 <- head(ordered_coefs, 10)
```

```{r, echo = FALSE}
library(knitr)


# Pretty table
kable(top10, caption = "Top Predicted Following Backwards Selection")
```

From the summary of the final model following backwards selection, we see that variables such as "Online" have a large negative coefficient, meaning they are inversely proportional to SPX. Variables such as "Book..Periodical..and.Music.Stores" have very large positive coefficients, meaning they are proportional to an increased SPX.

```{r, echo = FALSE}
AIC(model_backward)
```

```{r, echo = FALSE}
AIC(model2_lasso)
```

From our comparison, we actually see that the LASSO fitted model has a lower AIC than the backward selection model, meaning that the LASSO model has a better tradeoff between model fit and complexity.

\newpage
# Section 3. Logistic Regression

```{r, echo = FALSE}
df$SPX_up <- ifelse(df$SPX > dplyr::lag(df$SPX), 1, 0)
df <- df[complete.cases(df), ]  # Remove first row with NA from lag

log_model <- glm(SPX_up ~ ., data = dplyr::select(df, -SPX), family = binomial)

s <- summary(log_model)
coefs <- s$coefficients
coefs_no_intercept <- coefs[rownames(coefs) != "(Intercept)", ]
ordered_coefs <- coefs_no_intercept[order(coefs_no_intercept[, 4]), ]
top10 <- head(ordered_coefs, 10)
```

```{r, echo = FALSE}
library(knitr)

# Pretty table
kable(top10, caption = "Top Predicted Following Backwards Selection")
```
Our logistic regression has an AIC of 314, which we can compare to the boosted model coming next. 

Additionally, we evaluate the performance of the model, such as ROC curve and accuracy, more directly with statistics in Section 4, where we are able to directly compare performance with the boosted model. Overall, though, the model performs quite well, with an accuracy of around $88.6\%$. Additionally, we can seem to see some patterns in the regression, as it seems that there is much weight given to manufacturing related spending, as they make up 5 of the top 10 variables by significance in the regression. This focus on one industry paints an interesting picture, as we do not tend to think of manufacturing as extremely decisive towards the direction of the stock market—tech industries usually take that place. However, manufacturing has long been a pillar of the US economy, and often serves as a spending multiplier in poor economic times. Thus, this nuance could be something the model picked up on.

However, as we discuss later, the boosted model, which covered a more diverse array of variables, did end up performing better in the end. Thus, this focus on one industry could have potentially been detractive to the final performance of the Logistic Regression. We delve more into this comparative in Section 4, however.

\newpage
# Section 4. Boosting
```{r, echo = FALSE}
install.packages("gbm", repos = "http://cran.us.r-project.org")
library(gbm)

set.seed(20250331)

boost_model <- gbm(SPX_up ~ ., 
                   data = dplyr::select(df, -SPX), 
                   distribution = "bernoulli",
                   n.trees = 1000,
                   interaction.depth = 3,
                   shrinkage = 0.01,
                   cv.folds = 5)

# Get variable importance (relative influence)
var_importance <- invisible(summary(boost_model, plotit = TRUE))

# Extract top 10 variables
top10 <- head(var_importance[order(var_importance$rel.inf, decreasing = TRUE), ], 10)
```

```{r, echo = FALSE}
library(knitr)

# Pretty table
kable(top10, caption = "Top Predicted Following Backwards Selection")
```

```{r, echo = FALSE}
summary(boost_model, plotit = TRUE)
```

The variable importance graph highlights the most influential predictors in our Boosting model by measuring each variable's relative contribution to reducing classification error. Variables with higher importance scores had a greater impact on the model's ability to correctly predict whether the S&P 500 would increase on a given day.

```{r, echo = FALSE}

install.packages("pROC", repos = "http://cran.us.r-project.org")
install.packages("caret", repos = "http://cran.us.r-project.org")

library(pROC)
library(caret)

log_probs <- predict(log_model, type = "response")
boost_probs <- predict(boost_model, newdata = df, n.trees = 1000, type = "response")

roc_log <- roc(df$SPX_up, log_probs)
roc_boost <- roc(df$SPX_up, boost_probs)

plot(roc_log, col = "blue", main = "ROC Curve: Logistic vs Boosting")
lines(roc_boost, col = "red")
legend("bottomright", legend = c("Logistic", "Boosting"), col = c("blue", "red"), lwd = 2)
```

```{r, echo = FALSE}
auc_log <- auc(roc_log)
auc_boost <- auc(roc_boost)

log_preds <- ifelse(log_probs > 0.5, 1, 0)
boost_preds <- ifelse(boost_probs > 0.5, 1, 0)

cmLR <- confusionMatrix(factor(log_preds), factor(df$SPX_up), positive = "1")
cmBoost <- confusionMatrix(factor(boost_preds), factor(df$SPX_up), positive = "1")

cmLR
cmBoost
```

```{r, echo = FALSE}
library(knitr)

# Convert to data frame
lasso_selected_df <- data.frame(Selected_Variables = lasso_selected)

# Pretty table
kable(top10, caption = "Top Predicted Following Backwards Selection")
```

To evaluate the classification performance of our models, we compared logistic regression and gradient boosting using confusion matrices and standard metrics. The boosting model outperformed logistic regression across the board, achieving a higher overall accuracy (94.3% vs. 88.6%) and better balanced accuracy (0.908 vs. 0.850). Notably, boosting achieved perfect specificity, correctly identifying all instances where the S&P 500 did not increase, and demonstrated stronger sensitivity (0.815 vs. 0.754), indicating more accurate detection of positive market movements. The model also exhibited a perfect positive predictive value, meaning every predicted increase in the S&P 500 was correct. The results highlight that boosting overall was a better model at predicting whether SPX would increase. 

Indeed, as we discussed prior, boosting included a wider array of significant predictive variables, in terms of sector. Thus, this confirms our hypothesis that looking at the economy as a whole is necessary to understand how the SPX will move. Still, the Logistic Regression helped us understand one particular valuable set of predictors, namely variables relating to the manufacturing sector, as honing in on that sector performed only moderately worse than allowing for a wider, more complex variety in our predictors.  
